#+begin_export latex
\section{Introduction}
A major use case for \ty{fur} is the construction of PCR primers from
its output. Before using such primers \emph{in vitro}, it is wise to
estimate their sensitivity and specificity through digital PCR against
a large database. The program \ty{scop} scores primers by calculating
their \emph{in silico} sensitivity and specificity. It takes as input
one or more primers intended for one reaction mix. As additional input
it takes a set of target taxon IDs and a Blast database linked to the
NCBI taxonomy, for example \ty{nt}. It then returns the sensitivity of
the primer set,
\begin{equation}\label{eq:sn}
s_{\rm n}=\frac{t_{\rm p}}{t_{\rm p}+f_{\rm n}}.
\end{equation}
where $t_{\rm p}$ is the number of true positives and $f_{\rm n}$ the
number of false negatives. 

It also calculates the specificity as the fraction of true hits,
\begin{equation}\label{eq:sp}
s_{\rm p}=\frac{t_{\rm p}}{t_{\rm p}+f_{\rm p}},
\end{equation}
where $f_{\rm p}$ is the number of false positives.

In addition, \ty{scop} prints the true positives, false positives, and
the false negatives, if any, for further checking with the program
\ty{cops}, which corrects the primer scores obtained by \ty{scop}.

To construct an example run, change into the \ty{data} directory of
the repo, download a sample database, unpack it, and return to the
parent directory.
\begin{verbatim}
$ cd data
$ wget guanine.evolbio.mpg.de/fur/sample.tgz
$ tar -xvzf sample.tgz
$ cd ../
\end{verbatim}
Then change into the \ty{scop} directory and analyze the sample
primers in the file \ty{prim.fasta}, which should amplify all database
entries of the taxa listed in \ty{tarTax.txt}.
\begin{verbatim}
$ cd scop
$ ./scop -d ../data/sample -t tarTax.txt prim.fasta
\end{verbatim}
In this setup, the sensitivity of the tested primers is maxima, so
there are no false negatives, but there appear to be a large number of
false positives, leading to a specificity score of only 0.47.
\begin{verbatim}
PrimerSet:      prim.fasta
Sensitivity:    1
Specificity:    0.47
TruePositives:  CP051792.1 CP051830.1...
FalsePositives: CP032803.1 CP022050.2...
\end{verbatim}

\section{Implementation}
The outline of \ty{scop} contains hooks for imports, types, methods,
functions, and the logic of the main function.  \bpr{scop}{pr:sco}
#+end_export
#+begin_src go <<scop.go>>=
  package main
  import (
	  //<<Imports, Pr. \ref{pr:sco}>>
  )
  //<<Types, Pr. \ref{pr:sco}>>
  //<<Methods, Pr. \ref{pr:sco}>>
  func main() {
	  //<<Main function, Pr. \ref{pr:sco}>>
  }
#+end_src
#+begin_export latex
\epr In the main function we first prepare the error messages. Then we
set the usage, declare the options, parse the options, and parse the
input.
#+end_export
#+begin_src go <<Main function, Pr. \ref{pr:sco}>>=
  util.PrepareErrorMessages("scop")
  //<<Set usage, Pr. \ref{pr:sco}>>
  //<<Declare options, Pr. \ref{pr:sco}>>
  //<<Parse options, Pr. \ref{pr:sco}>>
  //<<Parse input, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
We import \ty{util}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "github.com/evolbioinf/fur/util"
#+end_src
#+begin_export latex
The usage consists of the actual usage message, an explanation of the
purpose of \ty{scop}, and an example command.
#+end_export
#+begin_src go <<Set usage, Pr. \ref{pr:sco}>>=
  u := "scop -d <db> -t <taxids.txt> [option]... [foo.fa]..."
  p := "Score primers by comparing them to a Blast database."
  e := "scop -d nt -t targets.txt prim.fa"
  clio.Usage(u, p, e)
#+end_src
#+begin_export latex
We import \ty{clio}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "github.com/evolbioinf/clio"
#+end_src
#+begin_export latex
We declare seven options, the first two of which are necessary for the
program to run, so we shall make them mandatory,
\begin{enumerate}
\item \ty{-d}: Blast database
\item \ty{-t}: file of target taxon IDs
\item \ty{-i}: maximum number of mismatches
\item \ty{-l}: maximum length of amplicon
\item \ty{-e}: E-value
\item \ty{-T}: number of threads
\item \ty{-v}: version
\end{enumerate}
#+end_export
#+begin_src go <<Declare options, Pr. \ref{pr:sco}>>=
  optD := flag.String("d", "", "Blast database")
  optT := flag.String("t", "", "file of target taxon IDs")
  optI := flag.Int("i", 5, "maximum number of mismatches")
  optL := flag.Int("l", 4000, "maximum length of amplicon")
  optE := flag.Float64("e", 10.0, "E-value")
  optTT := flag.Int("T", 0, "number of threads (default CPUs)")
  optV := flag.Bool("v", false, "version")
#+end_src
#+begin_export latex
We import \ty{flag}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "flag"
#+end_src
#+begin_export latex
We parse the options and first respond to \ty{-v}, as a request for the
version stops \ty{scop}. Then we check whether the mandatory options
of Blast database (\ty{-d}) and target taxon IDs (\ty{-t}) have been
set. If so, we look up the expected target accessions and respond to the
number of threads, (\ty{-T}).
#+end_export
#+begin_src go <<Parse options, Pr. \ref{pr:sco}>>=
  flag.Parse()
  //<<Respond to \ty{-v}, Pr. \ref{pr:sco}>>
  //<<Has \ty{-d} been set? Pr. \ref{pr:sco}>>
  //<<Has \ty{-t} been set? Pr. \ref{pr:sco}>>
  //<<Look up expected target accessions, Pr. \ref{pr:sco}>>
  //<<Respond to \ty{-T}, Pr, \ref{pr:sco}>>
#+end_src
#+begin_export latex
If the user requested the version, we print it.
#+end_export
#+begin_src go <<Respond to \ty{-v}, Pr. \ref{pr:sco}>>=
  if *optV {
	  util.Version()
  }
#+end_src
#+begin_export latex
If the user didn't supply a Blast database, we bail with a friendly
message.
#+end_export
#+begin_src go <<Has \ty{-d} been set? Pr. \ref{pr:sco}>>=
  if *optD == "" {
	  log.Fatal("please supply a Blast datbase")
  }
#+end_src
#+begin_export latex
We import \ty{log}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "log"
#+end_src
#+begin_export latex
Similarly, if the user didn't supply a file with target taxon IDs, we
bail with a friendly message.
#+end_export
#+begin_src go <<Has \ty{-t} been set? Pr. \ref{pr:sco}>>=
  if *optT == "" {
	  log.Fatal("please supply a file of target taxon IDs")
  }
#+end_src
#+begin_export latex
We store the expected target accessions in a string map and obtain
them by querying the Blast database and analyzing the query results.
#+end_export
#+begin_src go <<Look up expected target accessions, Pr. \ref{pr:sco}>>=
  etacc := make(map[string]bool)
  //<<Query Blast database, Pr. \ref{pr:sco}>>
  //<<Analyze query result, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
We query the Blast database by calling the program \ty{blastdbcmd}
such that it returns the accessions and title lines for entries that
belong to the target taxa. Here is an example command to achieve this,
\begin{verbatim}
blastdbcmd -db nt -taxidlist tarTax.txt -outfmt "%a %t"
\end{verbatim}
Note the output format, where \verb+%a+ is the accession and \verb+%t+
the title line. We construct this command and run it. Notice that we
construct the argument array by splitting the command string into its
constituent fields. However, since the output format takes as value a
composite string, we append that to the argument slice
separately. Then we run the command and check its error.
#+end_export
#+begin_src go <<Query Blast database, Pr. \ref{pr:sco}>>=
  tmpl := "blastdbcmd -db %s -taxidlist %s -outfmt "
  cs := fmt.Sprintf(tmpl, *optD, *optT)
  args := strings.Fields(cs)
  args = append(args, "%a %t")
  cmd := exec.Command("blastdbcmd")
  cmd.Args = args
  out, err := cmd.Output()
  util.Check(err)
#+end_src
#+begin_export latex
We import \ty{fmt}, \ty{strings}, and \ty{exec}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "fmt"
  "strings"
  "os/exec"
#+end_src
#+begin_export latex
We split the query output at the line breaks into entries of the Blast
database. We iterate over these entries and save the accessions of
``complete genomes''.
#+end_export
#+begin_src go <<Analyze query result, Pr. \ref{pr:sco}>>=
  cg := []byte("complete genome")
  entries := bytes.Split(out, []byte("\n"))
  for _, entry := range entries {
	  if bytes.Contains(entry, cg) {
		  acc := string(bytes.Fields(entry)[0])
		  etacc[acc] = true
	  }
  }
#+end_src
#+begin_export latex
We import \ty{bytes}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "bytes"
#+end_src
#+begin_export latex
If the user didn't set the number of threads, we set it to the number
of CPUs.
#+end_export
#+begin_src go <<Respond to \ty{-T}, Pr, \ref{pr:sco}>>=
  if *optTT == 0 {
	  (*optTT) = runtime.NumCPU()
  }
#+end_src
#+begin_export latex
We import \ty{runtime}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "runtime"
#+end_src
#+begin_export latex
The remaining tokens on the command line are taken as the names of
input files containing sets of primers. If there are none, we append
the file name ``-'', which is interpreted by Blast as the standard
input steam. We iterate over the files and analyze each one.
#+end_export
#+begin_src go <<Parse input, Pr. \ref{pr:sco}>>=
  primerSets := flag.Args()
  if len(primerSets) == 0 {
	  primerSets = append(primerSets, "-")
  }
  for _, primerSet := range primerSets {
	  //<<Analyze primer set, Pr. \ref{pr:sco}>>
  }
#+end_src
#+begin_export latex
To analyze a primer set, we run Blast, get the observed target
accessions from the Blast output, and compare them to the expected
target accessions. From this comparison we get the true positives,
false positives, and false negatives. This allows us to calculate the
sensitivity and specificity of our primer set according to
equations~(\ref{eq:sn}) and (\ref{eq:sp}), which we report.
#+end_export
#+begin_src go <<Analyze primer set, Pr. \ref{pr:sco}>>=
  //<<Run Blast, Pr. \ref{pr:sco}>>
  //<<Get observed target accessions, Pr. \ref{pr:sco}>>
  //<<Compare observed and expected target accessions, Pr. \ref{pr:sco}>>
  //<<Calculate sensitivity, Pr. \ref{pr:sco}>>
  //<<Calculate specificity, Pr. \ref{pr:sco}>>
  //<<Report sensitivity and specificity, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
We construct the Blast command for short queries like we constructed
the \ty{blastdbcmd} command. However, this time the command is called
\ty{blastn} and its task is called \ty{blastn-short}.

We are looking for hits where the alignment length is equal to the
primer length, that is, the query length, with no more than a maximum
number of mismatches. Any pairs of such hits are checked to see
whether they might form an amplicon by investigating their subject
accession and position. So as our output we request a table consisting
of alignment length, query length, the number of mismatches, the
subject accession, the subject start, and the subject end
(Table~\ref{tab:bout}). We run this command, store its output, and
check the error it returns.
\begin{table}
  \caption{The six columns of output in our run of \ty{blastn} for
    finding amplicons.}\label{tab:bout}
  \begin{center}
    \begin{tabular}{lll}
      \hline
      Col. & Name & Meaning\\\hline
      1 & \ty{length} & alignment length\\
      2 & \ty{qlen} & query length\\
      3 & \ty{mismatch} & number of mismatches\\
      4 & \ty{saccver} & subject accession with version\\
      5 & \ty{sstart} & start in subject\\
      6 & \ty{send} & end in subject\\\hline
    \end{tabular}
  \end{center}
\end{table}
#+end_export
#+begin_src go <<Run Blast, Pr. \ref{pr:sco}>>=
  tmpl = "blastn -task blastn-short -query %s -db %s " +
	  "-evalue %g -num_threads %d -outfmt "
  cs = fmt.Sprintf(tmpl, primerSet, *optD, *optE, *optTT)
  args = strings.Fields(cs)
  args = append(args, "6 length qlen mismatch " +
	  "saccver sstart send")
  cmd = exec.Command("blastn")
  cmd.Args = args
  out, err = cmd.CombinedOutput()
  //<<Check Blast error, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
If Blast returned an error, we print the output and quit.
#+end_export
#+begin_src go <<Check Blast error, Pr. \ref{pr:sco}>>=
  if err != nil {
	  log.Fatal(string(out))
  }
#+end_src
#+begin_export latex
We construct a map for storing the observed target accessions and a
slice of Blast results. Then we store the Blast hits, before we filter
them and look for amplicons.
#+end_export
#+begin_src go <<Get observed target accessions, Pr. \ref{pr:sco}>>=
  otacc := make(map[string]bool)
  hits := make([]*Hit, 0)
  //<<Store Blast hits, Pr. \ref{pr:sco}>>
  //<<Filter Blast hits, Pr. \ref{pr:sco}>>
  //<<Find amplicons, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
We declare a Blast hit to consist of the six fields listed in
Table~\ref{tab:bout}.
#+end_export
#+begin_src go <<Types, Pr. \ref{pr:sco}>>=
  type Hit struct {
	  length, qlen, mismatch int
	  saccver string
	  sstart, send int
  }
#+end_src
#+begin_export latex
We iterate over the primer sets of the Blast output and from every
line that consists of six fields, we extract the hit.
#+end_export
#+begin_src go <<Store Blast hits, Pr. \ref{pr:sco}>>=
  lines := bytes.Split(out, []byte("\n"))
  for _, line := range lines {
	  fields := bytes.Fields(line)
	  hit := new(Hit)
	  if len(fields) == 6 {
		  //<<Extract hit, Pr. \ref{pr:sco}>>
	  }
	  hits = append(hits, hit)
  }
#+end_src
#+begin_export latex
We convert the byte slices of a hit either to string or to integer. If
we convert to integer, we also check the error returned.
#+end_export
#+begin_src go <<Extract hit, Pr. \ref{pr:sco}>>=
  hit.length, err = strconv.Atoi(string(fields[0]))
  util.Check(err)
  hit.qlen, err = strconv.Atoi(string(fields[1]))
  util.Check(err)
  hit.mismatch, err = strconv.Atoi(string(fields[2]))
  util.Check(err)
  hit.saccver = string(fields[3])
  hit.sstart, err = strconv.Atoi(string(fields[4]))
  util.Check(err)
  hit.send, err = strconv.Atoi(string(fields[5]))
  util.Check(err)
#+end_src
#+begin_export latex
We import \ty{strconv}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "strconv"
#+end_src
#+begin_export latex
We retain hits with query length equal to the alignment length and
with no more than the maximum number of mismatches.
#+end_export
#+begin_src go <<Filter Blast hits, Pr. \ref{pr:sco}>>=
  i := 0
  for _, hit := range hits {
	  if hit.qlen == hit.length &&
		  hit.mismatch <= *optI {
		  hits[i] = hit
		  i++
	  }
  }
  hits = hits[:i]
#+end_src
#+begin_export latex
Amplicons are hits on the same subject where the 5'-hit is on the
forward strand and the 3'-hit on the reverse strand
(Figure~\ref{fig:pcr}). So we begin our search for amplicons by
ordering the hits.

\begin{figure}
  \footnotesize
  \input{../scop/primConfig}
  \normalsize
    \caption{Forward and reverse PCR primers, $p_{\rm f}$ and $p_{\rm
      r}$ (top panel), along the forward or reverse strands of a
    template, $t_{\rm f}$ and $t_{\rm r}$ (bottom
    panel).}\label{fig:pcr}
\end{figure}

In Blast, strandedness is encoded in the relationship between the
start and the end position of a hit. If the start is less than the
end, the hit is on the forward strand, if the start is greater than
the end, the hit is on the reverse strand. So we iterate over the
ordered hits and for each potential forward primer that hasn't yet
produced an amplicon look for a reverse primer.
#+end_export
#+begin_src go <<Find amplicons, Pr. \ref{pr:sco}>>=
  sort.Sort(HitSlice(hits))
  for i, hit := range hits {
	  if !otacc[hit.saccver] && hit.sstart < hit.send {
		  fp := hit
		  //<<Look for reverse primer, Pr. \ref{pr:sco}>>
	  }
  }
#+end_src
#+begin_export latex
We import \ty{sort}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:sco}>>=
  "sort"
#+end_src
#+begin_export latex
We declare the sortable type \ty{HitSlice}.
#+end_export
#+begin_src go <<Types, Pr. \ref{pr:sco}>>=
  type HitSlice []*Hit
#+end_src
#+begin_export latex
To allow sorting, we specify the three Methods required by the
\ty{Sort} interface, \ty{Len}, \ty{Swap}, and \ty{Less}. We begin with
\ty{Len} and \ty{Swap}.
#+end_export
#+begin_src go <<Methods, Pr. \ref{pr:sco}>>=
  func (h HitSlice) Len() int {
	  return len(h)
  }
  func (h HitSlice) Swap(i, j int) {
	  h[i], h[j] = h[j], h[i]
  }
#+end_src
#+begin_export latex
In \ty{Less} we sort by subject accession and within identical
subjects by start position.
#+end_export
#+begin_src go <<Methods, Pr. \ref{pr:sco}>>=
  func (h HitSlice) Less(i, j int) bool {
	  if h[i].saccver == h[j].saccver {
		  return h[i].sstart < h[j].sstart
	  }
	  return h[i].saccver < h[j].saccver
  }
#+end_src
#+begin_export latex
Reverse primers are located on the reverse strand within the range of
permissible amplicon lengths.
#+end_export
#+begin_src go <<Look for reverse primer, Pr. \ref{pr:sco}>>=
  for j := i+1; j < len(hits); j++ {
	  if hits[j].sstart > hits[j].send {
		  rp := hits[j]
		  if rp.send - fp.sstart + 1 <= *optL {
			  otacc[fp.saccver] = true
			  break
		  }
	  }
  }
#+end_src
#+begin_export latex
We now have the expected and the observed target accessions in hand
and compare them to calculate the number of true positives, $t_{\rm
  p}$, false positives, $f_{\rm p}$, and false negatives, $f_{\rm n}$.
#+end_export
#+begin_src go <<Compare observed and expected target accessions, Pr. \ref{pr:sco}>>=
  //<<Calculate $t_{\rm p}$, Pr. \ref{pr:sco}>>
  //<<Calculate $f_{\rm p}$, Pr. \ref{pr:sco}>>
  //<<Calculate $f_{\rm n}$, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
The true positives are the observed accessions that are also
expected. We count and save them.
#+end_export
#+begin_src go <<Calculate $t_{\rm p}$, Pr. \ref{pr:sco}>>=
  tp := 0
  truePositives := make([]string, 0)
  for o, _ := range otacc {
	  if etacc[o] {
		  truePositives = append(truePositives, o)
		  tp++
	  }
  }
#+end_src
#+begin_export latex
The false positives are the observed accessions that are not expected
are the false positives. We also save these accessions.
#+end_export
#+begin_src go <<Calculate $f_{\rm p}$, Pr. \ref{pr:sco}>>=
  fp := 0
  falsePositives := make([]string, 0)
  for o, _ := range otacc {
	  if !etacc[o] {
		  fp++
		  falsePositives = append(falsePositives, o)
	  }
  }
#+end_src
#+begin_export latex
The false negatives are the expected accessions that weren't
observed. We also save these accessions.
#+end_export
#+begin_src go <<Calculate $f_{\rm n}$, Pr. \ref{pr:sco}>>=
  fn := 0
  falseNegatives := make([]string, 0)
  for e, _ := range etacc {
	  if !otacc[e] {
		  fn++
		  falseNegatives = append(falseNegatives, e)
	  }
  }
#+end_src
#+begin_export latex
We calculate the sensitivity of our primer sample according to
equation~(\ref{eq:sn}).
#+end_export
#+begin_src go <<Calculate sensitivity, Pr. \ref{pr:sco}>>=
  sn := float64(tp) / (float64(tp) + float64(fn))
#+end_src
#+begin_export latex
We calculate the specificity of our primer sample according to
equation~(\ref{eq:sp}).
#+end_export
#+begin_src go <<Calculate specificity, Pr. \ref{pr:sco}>>=
  sp := float64(tp) / (float64(tp) + float64(fp))
#+end_src
#+begin_export latex
We report the name of our primer sample, its sensitivity, and its
specificity. In addition, we list the true positive, false positives
and false negatives.
#+end_export
#+begin_src go <<Report sensitivity and specificity, Pr. \ref{pr:sco}>>=
  fmt.Printf("PrimerSet:\t%s\n", primerSet)
  fmt.Printf("Sensitivity:\t%.2g\n", sn)
  fmt.Printf("Specificity:\t%.2g\n", sp)
  //<<Print true positives, Pr. \ref{pr:sco}>>
  //<<Print false positives, Pr. \ref{pr:sco}>>
  //<<Print false negatives, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
We sort the true positives to make their order reproducible, and list
them as a blank-delimited row.
#+end_export
#+begin_src go <<Print true positives, Pr. \ref{pr:sco}>>=
  if len(truePositives) > 0 {
	  sort.Strings(truePositives)
	  fmt.Printf("TruePositives:\t%s", truePositives[0])
	  for i := 1; i < tp; i++ {
		  fmt.Printf(" %s", truePositives[i])
	  }
	  fmt.Printf("\n")
  }
#+end_src
#+begin_export latex
We import \ty{sort}.
#+end_export
#+begin_export latex
We also print the sorted false positives as a blank-delimited row.
#+end_export
#+begin_src go <<Print false positives, Pr. \ref{pr:sco}>>=
  if len(falsePositives) > 0 {
	  sort.Strings(falsePositives)
	  fmt.Printf("FalsePositives:\t%s", falsePositives[0])
	  for i := 1; i < fp; i++ {
		  fmt.Printf(" %s", falsePositives[i])
	  }
	  fmt.Printf("\n")
  }
#+end_src
#+begin_export latex
We finally list the sorted false negatives as a blank-delimited row.
#+end_export
#+begin_src go <<Print false negatives, Pr. \ref{pr:sco}>>=
  if len(falseNegatives) > 0 {
	  sort.Strings(falseNegatives)
	  fmt.Printf("FalseNegatives:\t%s", falseNegatives[0])
	  for i := 1; i < fn; i++ {
		  fmt.Printf(" %s", falseNegatives[i])
	  }
	  fmt.Printf("\n")
  }
#+end_src
#+begin_export latex
We are finished with \ty{scop}, time to test it.
\section{Testing}
Our test for \ty{scop} contains hooks for imports and the testing
logic.
#+end_export
#+begin_src go <<scop_test.go>>=
  package main

  import (
	  "testing"
	  //<<Testing imports, Pr. \ref{pr:sco}>>
  )
  func TestScop(t *testing.T) {
	  //<<Testing, Pr. \ref{pr:sco}>>
  }
#+end_src
#+begin_export latex
We construct one test and run it.
#+end_export
#+begin_src go <<Testing, Pr. \ref{pr:sco}>>=
  //<<Construct test, Pr. \ref{pr:sco}>>
  //<<Run test, Pr. \ref{pr:sco}>>
#+end_src
#+begin_export latex
We use \ty{scop} to score the primers in \ty{prim.fasta} using the
sample database
\begin{verbatim}
../data/sample
\end{verbatim}
and the target taxa in \ty{tarTax.txt}.
#+end_export
#+begin_src go <<Construct test, Pr. \ref{pr:sco}>>=
  test := exec.Command("./scop", "-d", "../data/sample",
	  "-t", "tarTax.txt", "prim.fasta")
#+end_src
#+begin_export latex
We import \ty{exec}.
#+end_export
#+begin_src go <<Testing imports, Pr. \ref{pr:sco}>>=
  "os/exec"
#+end_src
#+begin_export latex
We run the test and compare the result we get to the result we want,
which is contained in the file \ty{r.txt}.
#+end_export
#+begin_src go <<Run test, Pr. \ref{pr:sco}>>=
  get, err := test.Output()
  if err != nil {
	  t.Error(err)
  }
  want, err := os.ReadFile("r.txt")
  if err != nil {
	  t.Error(err)
  }
  if !bytes.Equal(get, want) {
	  t.Errorf("get:\n%s\nwant:\n%s\n", get, want)
  }
#+end_src
#+begin_export latex
We import \ty{os} and \ty{bytes}.
#+end_export
#+begin_src go <<Testing imports, Pr. \ref{pr:sco}>>=
  "os"
  "bytes"
#+end_src
