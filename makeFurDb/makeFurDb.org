#+begin_export latex
\section{Introduction}
The program \ty{makeFurDb} takes as input a directory of target files
and a directory of neighbor files. From these two directories it
constructs a third, the \ty{fur} database. This ``database'' is then
analyzed with \ty{fur} to discover regions common to the targets that
are absent from the neighbors.

\ty{MakeFurDb} picks a target representative, $r$, by default the
shortest target. It then calculates the match lengths at every
position in $r$ with respect to $N$. For example, let
\[
r=\ty{AAATAATATT}
\]
and let $N$ consist of a single entry to start with,
\[
N=\{n_1\},
\]
where
\[
n_1=\ty{AATTTAAATT}.
\]
We start at the first position in $r$ and match it as far as possible
against $n_1$ to get the match factor
\[
r[1..4]=n_1[6..9]=\ty{AAAT}.
\]
Then we delete the match from $r$ to get $r=\ty{AATATT}$, and repeat
to find the two remaining match factors \ty{AAT} and \ty{ATT}. When
analyzing real data, $r$ and the elements of $N$ can have any size and
we also match the reverse complement of $r$.

Real genome assemblies may be scattered across many short contigs. To
prevent the computation of a suffix array for each of them, we
aggregate nucleotides from multiple contigs into pseudo contigs of
some minimal length, say 10 Mb.

The rationale of our matching step is that regions in $r$ with short
matches are absent from $N$. For neighborhoods with more than one
element, we match each sequence in turn. To see how this sequential
matching works, we add a second sequence to $N$,
\[
N=\{n_1, n_2\},
\]
where
\[
n_2=\ty{TAATAATATT}.
\]
Again, we match $r$ against $n_1$ as described, only this time we also
save the match lengths at their start positions in the array $\mbox{ml}$,
which is initialized to zero. So after the first round of matching we
have
\begin{center}
  \begin{tabular}{ccccccccccc}
    $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 0\\
    $r$ & \ty{A} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{T} & \ty{T}\\
    $\mbox{ml}$ & 4 & 0 & 0 & 0 & 3 & 0 & 0 & 3 & 0 & 0
  \end{tabular}
\end{center}

When we match $r$ against $n_2$, we find the match factors \ty{AA} and
\ty{ATAATATT} of lengths 2 and 8. We consider the length of each
factor. If it is larger than the corresponding entry in $\mbox{ml}$,
we update the entry. So in our example we don't update
$\mbox{ml}[1]=4$, because $4>2$, but we do update
$\mbox{ml}[3]\leftarrow 8$ to get
\begin{center}
  \begin{tabular}{ccccccccccc}
    $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 0\\
    $r$ & \ty{A} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{T} & \ty{T}\\
    $\mbox{ml}$ & 4 & 0 & 8 & 0 & 3 & 0 & 0 & 3 & 0 & 0
  \end{tabular}
\end{center}

For each comparison between $r$ and $n_i$, a new enhanced suffix array
is constructed for $n_i$. Algorithm~\ref{alg:ml} summarizes the
computation of match lengths.
\begin{algorithm}
  \caption{Computing match lengths between $r$ and
    neighborhood.}\label{alg:ml}
  \begin{algorithmic}[1]
    \input{../makeFurDb/mlAlg}
  \end{algorithmic}
\end{algorithm}

Once we've compared $r$ to each neighbor
sequence, we merge the results from the separate scans by traversing
$\mbox{ml}$ and entering at each position the implied maximal match
length. The implied match length is one less than the entry to its
left, unless the current entry is greater. Lines 1--4 of
Algorithm~\ref{alg:en} summarize the computation of the implied match
lengths. For our example we get,
\begin{algorithm}
  \caption{Converting match lengths to end positions.}\label{alg:en}
  \begin{algorithmic}[1]
    \input{../makeFurDb/endAlg}
  \end{algorithmic}
\end{algorithm}
\begin{center}
  \begin{tabular}{ccccccccccc}
    $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 0\\
    $r$ & \ty{A} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{T} & \ty{T}\\
    $\mbox{ml}$ & 4 & 3 & 8 & 7 & 6 & 5 & 4 & 3 & 2 & 1
  \end{tabular}
\end{center}

The number of match factors is now the number of steps required to
traverse $r$. Starting at the beginning of $l$, we skip to position
$1+4-1=4$ and from there to position $4+7-1=10$, the end of $r$. Thus we
have two match factors when comparing $r$ to $N$.

Instead of the total number of match factors, we are usually more
interested in the local density of match factors. This is quantified
in \ty{fur} through a sliding window analysis. To simplify the sliding
window analysis, we construct an array of end positions, $\mbox{en}[i]=1$ if a
match ends at $i$, $\mbox{en}[i]=0$ otherwise, as summarized in lines
6--11 of Algorithm~\ref{alg:en}. So in our example we have,
\begin{center}
  \begin{tabular}{ccccccccccc}
    $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 0\\
    $r$ & \ty{A} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{A} & \ty{T} & \ty{A} & \ty{T} & \ty{T}\\
    $\mbox{ml}$ & 4 & 3 & 8 & 7 & 6 & 5 & 4 & 3 & 2 & 1\\
    $\mbox{en}$ & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1
  \end{tabular}
\end{center}

The array $\mbox{en}$ is now ready for the sliding window analysis
with \ty{fur} to discover the regions with a high density of match
factors, which are the regions in $r$ absent from $N$. The strategy
for this is described in the Introduction and summarized there in
steps~(\ref{eq:fur1}) to (\ref{eq:fur3}). To enable \ty{fur} to carry
out these steps, \ty{makeFurDb} constructs a directory, or
``database'', that contains six elements,
\begin{enumerate}
\item a file containing the program version, \ty{v.txt}
\item a file containing the target representative, \ty{r.fasta}
\item a directory containing all targets minus the target
  representative, \ty{t/}
\item a file containing the end positions of the match factors of the
  target representative, \ty{e.fasta}
\item a Blast database of the neighbors, \ty{n}
\item a file containing the combined length of the neighbor sequences
  and their GC content, \ty{n.txt}
\end{enumerate}
The implementation of \ty{makeFurDb} is structured along the
construction of these six parts of the database. Of these six parts,
constructing the file of end positions is by far the most
time-consuming. We parallelize it along neighbor files, which means
that if we have, say, two threads, half the neighbor files are
processed by the first thread, the other half by the second. Once the
threads are finished, the match lengths from each thread are merged.

\section{Implementation}
Our outline of \ty{makeFurDb} contains hooks for imports, functions,
and the logic of the main function.  \bpr{makeFurDb}{pr:mfd}
#+end_export
#+begin_src go <<makeFurDb.go>>=
  package main

  import (
	  //<<Imports, Pr. \ref{pr:mfd}>>
  )
  //<<Functions, Pr. \ref{pr:mfd}>>
  func main() {
	  //<<Main function, Pr. \ref{pr:mfd}>>
  }
#+end_src
#+begin_export latex
\epr In the main function we prepare the error handling, declare the
options, set the usage, parse the options, and construct the \ty{fur}
database.
#+end_export
#+begin_src go <<Main function, Pr. \ref{pr:mfd}>>=
  //<<Prepare error handling, Pr. \ref{pr:mfd}>>
  //<<Declare options, Pr. \ref{pr:mfd}>>
  //<<Set usage, Pr. \ref{pr:mfd}>>
  //<<Parse options, Pr. \ref{pr:mfd}>>
  //<<Construct \ty{fur} database, Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
We prepare error handling by calling the utility function
\ty{PrepareErrorMessages}.
#+end_export
#+begin_src go <<Prepare error handling, Pr. \ref{pr:mfd}>>=
  util.PrepareErrorMessages("makeFurDb")
#+end_src
#+begin_src latex
  We declare options for printing the version, for picking the
  directories holding the targets, the neighbors, and the database, for
  picking the target representative, for allowing an existing database
  to be overwritten, and for setting the number of threads. By default,
  the number of threads is set to the number of logical CPUs available
  on the system, which should make \ty{makeFurDb} as fast as
  possible. However, the memory consumption of \ty{makeFurDb} also grows
  with the number of threads, and there might not be enough memory to
  run on all logical CPUs. So the user can trade speed for memory and
  opt for a smaller number of threads.
#+end_src
#+begin_src go <<Declare options, Pr. \ref{pr:mfd}>>=
  optV := flag.Bool("v", false, "version")
  optT := flag.String("t", "", "target directory")
  optN := flag.String("n", "", "neighbor directory")
  optD := flag.String("d", "", "database directory")
  optR := flag.String("r", "", "target representative " +
	  "(default shortest)")
  optO := flag.Bool("o", false, "overwrite existing database")
  optTT := flag.Int("T", 0, "threads (default all processors)")
#+end_src
#+begin_src latex
  We import \ty{flag}.
#+end_src
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "flag"
#+end_src
#+begin_export latex
By default \ty{makeFurDb} sets sequence data to upper case and then
removes any character that isn't \ty{A}, \ty{C}, \ty{G}, or \ty{T}. In
most cases these non-canonical nucleotides are stretches of \ty{N}s,
which cannot be used in primer design, the original purpose of Fur.

The disadvantage of removing non-canonical nucleotides is that this
leads to differences between the coordinates in \ty{r.fasta} and its
original. Annotations like genes are given with respect to this
original. To preserve their correctness, we also add an option of
keeping non-canonical nucleotides.
#+end_export
#+begin_src go <<Declare options, Pr. \ref{pr:mfd}>>=
  optK := flag.Bool("k", false, "keep non-canonical nucleotides")
#+end_src
#+begin_export latex
The usage consists of three parts, the actual usage message, an
explanation of the purpose of \ty{makeFurDb}, and an example command.
#+end_export
#+begin_src go <<Set usage, Pr. \ref{pr:mfd}>>=
  u := "makeFurDb [option]... -t <targetDir> " +
	  "-n <neighborDir> -d <db>"
  p := "Construct fur database."
  e := "makeFurDb -t targets/ -n neighbors/ -d fur.db"
  clio.Usage(u, p, e)
#+end_src
#+begin_export latex
We import \ty{clio}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "github.com/evolbioinf/clio"
#+end_src
#+begin_export latex
We parse the options and respond to the version option, \ty{-v}, the
three directory options, \ty{-t}, \ty{-n}, \ty{-d}, and the number of
threads, \ty{-T}.
#+end_export
#+begin_src go <<Parse options, Pr. \ref{pr:mfd}>>=
  flag.Parse()
  //<<Respond to \ty{-v}, Pr. \ref{pr:mfd}>>
  //<<Respond to \ty{-t}, Pr. \ref{pr:mfd}>>
  //<<Respond to \ty{-n}, Pr. \ref{pr:mfd}>>
  //<<Respond to \ty{-d}, Pr. \ref{pr:mfd}>>
  //<<Respond to \ty{-T}, Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
If the user requested the version, we call the utility function
\ty{PrintInfo} with the program name.
#+end_export
#+begin_src go <<Respond to \ty{-v}, Pr. \ref{pr:mfd}>>=
  if *optV {
	  util.PrintInfo("makeFurDb")
  }
#+end_src
#+begin_export latex
We import \ty{util}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "github.com/evolbioinf/fur/util"
#+end_src
#+begin_export latex
If the user didn't set a targets directory, we exit with a friendly
message.
#+end_export
#+begin_src go <<Respond to \ty{-t}, Pr. \ref{pr:mfd}>>=
  if *optT == "" {
	  m := "please provide a directory " +
		  "of target sequences"
	  fmt.Fprintf(os.Stderr, "%s\n", m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
We import \ty{fmt} and \ty{os}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "fmt"
  "os"
#+end_src
#+begin_export latex
We do the same if the user didn't set a neighbors directory.
#+end_export
#+begin_src go <<Respond to \ty{-n}, Pr. \ref{pr:mfd}>>=
  if *optN == "" {
	  m := "please provide a directory " +
		  "of neighbor sequences"
	  fmt.Fprintf(os.Stderr, "%s\n", m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
The database directory is the last directory the user needs to set. If
the user failed to do so, we abort with a friendly message. If, on the
other hand, the user named an existing database, we deal with it. At
the end we construct the empty database directory that we fill in the
following sections.
#+end_export
#+begin_src go <<Respond to \ty{-d}, Pr. \ref{pr:mfd}>>=
  if *optD == "" {
	  m := "please provide a database name"
	  fmt.Fprintf(os.Stderr, "%s\n", m)
	  os.Exit(1)
  } else {
	  _, err := os.Stat(*optD)
	  if err == nil {
		  //<<Deal with existing database, Pr. \ref{pr:mfd}>>
	  }
	  err = os.Mkdir(*optD, 0750)
	  util.Check(err)
  }

#+end_src
#+begin_export latex
If the user allowed the database to be overwritten, we remove
it. Otherwise, we bail with a friendly message.
#+end_export
#+begin_src go <<Deal with existing database, Pr. \ref{pr:mfd}>>=
  if *optO {
	  err := os.RemoveAll(*optD)
	  util.Check(err)
  } else {
	  m := fmt.Sprintf("database %s already exists", *optD)
	  fmt.Fprintf(os.Stderr, "%s\n", m)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
If the user set a negative number of threads, something is wrong and
we throw a fatal error. If the user didn't set the number of threads,
we use the number of logical CPUs available.
#+end_export
#+begin_src go <<Respond to \ty{-T}, Pr. \ref{pr:mfd}>>=
  if *optTT < 0 {
	  log.Fatalf("Can't set %d threads.", *optTT)
  }
  if *optTT == 0 {
	  (*optTT) = runtime.NumCPU()
  }
#+end_src
#+begin_export latex
We import \ty{log} and \ty{runtime}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "log"
  "runtime"
#+end_src
#+begin_export latex
To construct the \ty{fur} database, we read the names of the targets
and neighbors and make sure the two lists don't overlap. This is best
done by storing the names of the targets and neighbors in
maps. However, map keys are accessed randomly. To impose order on
them, we sort the names before making sure they don't overlap. Then we
construct the six elements of the \ty{fur} database to store the
version, the target representative, the other targets, the ends of the
match factors, the blast database of neighbors, and the statistics of
the neighbor sequences. These statistics are later used by \ty{fur} to
calculate the null distribution of match lengths.
#+end_export
#+begin_src go <<Construct \ty{fur} database, Pr. \ref{pr:mfd}>>=
  //<<Read names of targets and neighbors, Pr. \ref{pr:mfd}>>
  //<<Sort names of targets and neighbors, Pr. \ref{pr:mfd}>>
  //<<Do targets and neighbors overlap? Pr. \ref{pr:mfd}>>
  //<<Write version to \ty{v.txt}, Pr. \ref{pr:mfd}>>
  //<<Write target representative to \ty{r.fasta}, Pr. \ref{pr:mfd}>>
  //<<Write the other targets to \ty{t/}, Pr. \ref{pr:mfd}>>
  //<<Write match factors ends to \ty{e.fasta}, Pr. \ref{pr:mfd}>>
  //<<Write Blast database of neighbors to \ty{n}, Pr. \ref{pr:mfd}>>
  //<<Write statistics of neighbors to \ty{n.txt}, Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
We delegate reading the names of the targets and neighbors to the
function \ty{readDir}. If either of these directories is empty, we
bail with a friendly message.
#+end_export
#+begin_src go <<Read names of targets and neighbors, Pr. \ref{pr:mfd}>>=
  targets := readDir(*optT)
  if len(targets) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optT)
	  os.Exit(1)
  }
  neighbors := readDir(*optN)
  if len(neighbors) == 0 {
	  fmt.Fprintf(os.Stderr, "%s is empty\n", *optN)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
The function \ty{readDir} takes as input the name of a directory and
returns a map of the names of the sequence files contained in the
directory. We check each directory entry before storing its name.
#+end_export
#+begin_src go <<Functions, Pr. \ref{pr:mfd}>>=
  func readDir(dir string) map[string]bool {
	  dirEntries, err := os.ReadDir(dir)
	  util.Check(err)
	  names := make(map[string]bool)
	  for _, dirEntry := range dirEntries {
		  //<<Check directory entry, Pr. \ref{pr:mfd}>>
		  names[dirEntry.Name()] = true
	  }
	  return names
  }
#+end_src
#+begin_export latex
A directory entry should be a file rather than a subdirectory. Given
that it is indeed a file, it should also be a nucleotide FASTA file.
#+end_export
#+begin_src go <<Check directory entry, Pr. \ref{pr:mfd}>>=
  //<<Is the directory entry a file? Pr. \ref{pr:mfd}>>
  //<<Is the directory entry a nucleotide file? Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
If the directory entry is a subdirectory, we warn the user and skip
it.
#+end_export
#+begin_src go <<Is the directory entry a file? Pr. \ref{pr:mfd}>>=
  if dirEntry.IsDir() {
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr,
		  "skipping subdirectory %s\n", p)
	  continue
  }
#+end_src
#+begin_export latex
There are five filename extensions that denote nucleotide FASTA files
listed on
Wikipedia\footnote{\ty{https://en.wikipedia.org/wiki/FASTA\_format}},
\ty{.fasta}, \ty{.fna}, \ty{.ffn}, \ty{.frn}, and \ty{.fa}. If the
file has a different extension or none, we skip it and send a warning.
#+end_export
#+begin_src go <<Is the directory entry a nucleotide file? Pr. \ref{pr:mfd}>>=
  ext := filepath.Ext(dirEntry.Name())
  if ext != ".fasta" && ext != ".fna" && ext != ".ffn" &&
	  ext != ".frn" && ext != ".fa" {
	  m := "%s doesn't have the extension of " +
		  "a nucleotide FASTA file; skipping it\n"
	  p := dir + "/" + dirEntry.Name()
	  fmt.Fprintf(os.Stderr, m, p)
	  continue
  }
#+end_src
#+begin_export latex
We import \ty{filepath}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "path/filepath"
#+end_src
#+begin_export latex
We store the names of the neighbors and targets in slices and sort
them.
#+end_export
#+begin_src go <<Sort names of targets and neighbors, Pr. \ref{pr:mfd}>>=
  var targetNames, neighborNames []string
  for target := range targets {
	  targetNames = append(targetNames, target)
  }
  sort.Strings(targetNames)
  for neighbor := range neighbors {
	  neighborNames = append(neighborNames, neighbor)
  }
  sort.Strings(neighborNames)
#+end_src
#+begin_export latex
We import \ty{sort}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "sort"
#+end_src
#+begin_export latex
A common error when searching for markers is to include a sequence
file in both the target and the neighbor set. To guard against this,
we search for pairs of repeated file names and bail with message if we
find one.
#+end_export
#+begin_src go <<Do targets and neighbors overlap? Pr. \ref{pr:mfd}>>=
  for _, target := range targetNames {
	  if neighbors[target] {
		  m := "found %s/%s and %s/%s; please " +
			  "make sure the targets and " +
			  "neighbors don't overlap"
		  fmt.Fprintf(os.Stderr, m, *optT,
			  target, *optN, target)
		  os.Exit(1)
	  }
  }
#+end_src
#+begin_export latex
\subsection{Version File, \ty{v.txt}}
To construct the version file, we open \ty{v.txt} inside the database,
write the version to it, and close it again.
#+end_export
#+begin_src go <<Write version to \ty{v.txt}, Pr. \ref{pr:mfd}>>=
  vf := *optD + "/v.txt"
  f, err := os.Create(vf)
  util.Check(err)
  fmt.Fprintf(f, "%s\n", util.Version())
  f.Close()
#+end_src
#+begin_export latex
\subsection{Target Representative, \ty{r.fasta}}
If the user didn't set a target representative, we need to pick
one. In any case, we report which target is used as
representative. Then we read the target representative and write it.'
#+end_export
#+begin_src go <<Write target representative to \ty{r.fasta}, Pr. \ref{pr:mfd}>>=
  if *optR == "" {
	  //<<Pick target representative, Pr. \ref{pr:mfd}>>
  }
  fmt.Fprintf(os.Stderr, "using %s as target representative\n",
	  (*optR))
  //<<Read target representative, Pr. \ref{pr:mfd}>>
  //<<Write target representative, Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
By default, the target representative is the shortest target
sequence. To pick it, we iterate over the target files, calculate
their lengths, and store the name of the shortest target found so far
and its length. At the end we store the name of the overall shortest
target.
#+end_export
#+begin_src go <<Pick target representative, Pr. \ref{pr:mfd}>>=
  minTar := ""
  minLen := math.MaxInt
  for target, _ := range targets {
	  l := 0
	  //<<Calculate length of target sequence, Pr. \ref{pr:mfd}>>
	  if l < minLen {
		  minTar = target
		  minLen = l
	  }
  }
  (*optR) = minTar
#+end_src
#+begin_export latex
We import \ty{math}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "math"
#+end_src
#+begin_export latex
We iterate over the entries in the current target file and sum their
lengths.
#+end_export
#+begin_src go <<Calculate length of target sequence, Pr. \ref{pr:mfd}>>=
  p := *optT + "/" + target
  f, err := os.Open(p)
  util.Check(err)
  defer f.Close()
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  l += len(sc.Sequence().Data())
  }
#+end_src
#+begin_export latex
We import \ty{fasta}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "github.com/evolbioinf/fasta"
#+end_src
#+begin_export latex
We reserve space for the forward and reverse strands of the sequences
that make up the target representative. Then we iterate over the
sequences, clean, and store them.
#+end_export
#+begin_src go <<Read target representative, Pr. \ref{pr:mfd}>>=
  var repSeqs, revRepSeqs []*fasta.Sequence
  p = *optT + "/" + *optR
  f , err = os.Open(p)
  util.Check(err)
  defer f.Close()
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  seq := sc.Sequence()
	  //<<Clean sequence, Pr. \ref{pr:mfd}>>
	  //<<Store sequence, Pr. \ref{pr:mfd}>>
  }
#+end_src
#+begin_export latex
Cleaning consists of setting the sequence data to upper case. We also
remove non-canonical nucleotides, unless the user opted to keep
them. In the last cleaning step we reduce the header to the first
token, usually the accession. This ensures that there are no blanks in
the names of the sequences later passed to Blast.
#+end_export
#+begin_src go <<Clean sequence, Pr. \ref{pr:mfd}>>=
  d := bytes.ToUpper(seq.Data())
  if !*optK {
	  //<<Remove non-canonical nucleotides, Pr. \ref{pr:mfd}>>
  }
  h := strings.Fields(seq.Header())[0]
  seq = fasta.NewSequence(h, d)
#+end_src
#+begin_export latex
We import \ty{bytes} and \ty{strings}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "bytes"
  "strings"
#+end_src
#+begin_export latex
We remove the non-canonical bases using reslicing.
#+end_export
#+begin_src go <<Remove non-canonical nucleotides, Pr. \ref{pr:mfd}>>=
  i := 0
  for _, c := range d {
	  if c == 'A' || c == 'C' || c == 'G' || c == 'T' {
		  d[i] = c
		  i++
	  }
  }
  d = d[:i]
#+end_src
#+begin_export latex
We store the forward strand, construct the reverse strand from it, and
store that, too.
#+end_export
#+begin_src go <<Store sequence, Pr. \ref{pr:mfd}>>=
  repSeqs = append(repSeqs, seq)
  seq = fasta.NewSequence(seq.Header(), seq.Data())
  seq.ReverseComplement()
  revRepSeqs = append(revRepSeqs, seq)
#+end_src
#+begin_export latex
We write the target representative to \ty{r.fasta}.
#+end_export
#+begin_src go <<Write target representative, Pr. \ref{pr:mfd}>>=
  f, err = os.Create(*optD + "/r.fasta")
  util.Check(err)
  defer f.Close()
  for _, repSeq := range repSeqs {
	  fmt.Fprintf(f, "%s\n", repSeq)
  }
#+end_src
#+begin_export latex
\subsection{Target Directory, \ty{t/}}
We copy the targets minus the representative to the directory \ty{t}
inside the database.
#+end_export
#+begin_src go <<Write the other targets to \ty{t/}, Pr. \ref{pr:mfd}>>=
  p = *optD + "/t/"
  err = os.Mkdir(p, 0750)
  util.Check(err)
  for target, _ := range targets {
	  if target == *optR { continue }
	  source := *optT + "/" + target
	  dest := *optD + "/t/" + target
	  sd, err := os.ReadFile(source)
	  util.Check(err)
	  err = os.WriteFile(dest, sd, 0666)
  }
#+end_src
#+begin_export latex
\subsection{End Positions, \ty{e.fasta}}
We now turn to the computation of the ends of match factors, the
algorithmic heart of \ty{makeFurDb}. We begin by reading the sequences
that make up the target representative, or rep. We find the match
lengths by streaming the target rep against the neighbors. These
neighbors may be scattered over many short contigs. Na√Øvely, we might
build one suffix array per contig, however short, but it is more
efficient to build a suffix array for one long sequence than multiple
suffix arrays for its constituent substrings. So before we stream the
target rep against a neighbor suffix array, we determine the maximum
neighbor length. Double this maximum length is the upper bound of
neighbor nucleotides aggregated during the streaming step. Streaming
is the rate-limiting step in \ty{makeFurDb} and we distribute it among
goroutines.

Once the target rep has been streamed against all neighbors, the
resulting match lengths are merged. Finally, the merged match lengths
are converted to match ends and written to file.
#+end_export
#+begin_src go <<Write match factors ends to \ty{e.fasta}, Pr. \ref{pr:mfd}>>=
  //<<Read sequences of target rep, Pr. \ref{pr:mfd}>>
  //<<Find maximum neighbor length, Pr. \ref{pr:mfd}>>
  //<<Distribute streaming among goroutines, Pr. \ref{pr:mfd}>>
  //<<Merge match lengths, Pr. \ref{pr:mfd}>>
  //<<Convert match lengths to match ends, Pr. \ref{pr:mfd}>>
  //<<Write match ends, Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
We open the target rep file inside the database, store the forward and
reverse strands of sequences it contains, and close the file again.
#+end_export
#+begin_src go <<Read sequences of target rep, Pr. \ref{pr:mfd}>>=
  f, err = os.Open(*optD + "/r.fasta")
  util.Check(err)
  var targetSeqs, revTargetSeqs []*fasta.Sequence
  sc = fasta.NewScanner(f)
  for sc.ScanSequence() {
	  s := sc.Sequence()
	  targetSeqs = append(targetSeqs, s)
	  s = fasta.NewSequence(s.Header(), s.Data())
	  s.ReverseComplement()
	  revTargetSeqs = append(revTargetSeqs, s)
  }
  f.Close()
#+end_src
#+begin_export latex
We iterate over the neighbor files and for each file we iterate over
the sequences it contains to find the length of the longest.
#+end_export
#+begin_src go <<Find maximum neighbor length, Pr. \ref{pr:mfd}>>=
  mnl := -1
  for _, neighbor := range neighborNames {
	  p := *optN + "/" + neighbor
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Interate over neighbor sequences, Pr. \ref{pr:mfd}>>
	  f.Close()
  }
#+end_src
#+begin_export latex
We use a scanner to iterate over the sequences in the current neighbor
file.
#+end_export
#+begin_src go <<Interate over neighbor sequences, Pr. \ref{pr:mfd}>>=
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  l := len(sc.Sequence().Data())
	  if l > mnl {
		  mnl = l
	  }
  }
#+end_src
#+begin_export latex
We distribute the streaming work along sets of neighbor files. So we
split the list of neighbor files into packages of size \ty{n}. Then we
concurrently stream the target representative against the sets of
neighbor files. For this we use a concurrency pattern based on a
\ty{sync.WaitGroup}, which we can think of as a counter of working
goroutines. This concurrency pattern is built from a channel and a
\ty{WaitGroup}, both of which we initialize outside of the loop in
which we spawn the goroutines. Inside the loop we increment the
counter of the working goroutines and construct the goroutine for
streaming. After the loop, we construct the closer and iterator that
belong to the \ty{WaitGroup} concurrency pattern, as illustrated in
Figure 8.5 of \cite[p. 239]{don16:go}.
#+end_export
#+begin_src go <<Distribute streaming among goroutines, Pr. \ref{pr:mfd}>>=
  neighborNameSets := make([][]string, 0)
  //<<Split neighbor file names, Pr. \ref{pr:mfd}>>
  lengthSets := make(chan [][]int)
  var wg sync.WaitGroup
  for _, neighborNames := range neighborNameSets {
	  wg.Add(1)
	  //<<Goroutine for streaming, Pr. \ref{pr:mfd}>>
  }
  //<<Streaming closer, Pr. \ref{pr:mfd}>>
  //<<Streaming iterator, Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
We import \ty{sync}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "sync"
#+end_src
#+begin_export latex
Given $n$ neighbor files and $T$ threads, we split the file names into
slices of length $\ell=n/T$.
#+end_export
#+begin_src go <<Split neighbor file names, Pr. \ref{pr:mfd}>>=
  n := len(neighborNames)
  length := int(math.Ceil(float64(n)/float64(*optTT)))
  start := 0
  end := length
  for start < n {
	  neighborNameSets = append(neighborNameSets,
		  neighborNames[start:end])
	  start = end
	  end += length
  }
#+end_src
#+begin_export latex
We import \ty{math}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "math"
#+end_src
#+begin_export latex
We pass the current set of neighbor names to the goroutine. Inside the
goroutine, we make sure it is eventually closed again. We also declare
and allocate a variable for holding the match lengths, stream the
target representative against the neighbors, and pass the resulting
match lengths along the channel.
#+end_export
#+begin_src go <<Goroutine for streaming, Pr. \ref{pr:mfd}>>=
  go func(neighborNames []string) {
	  defer wg.Done()
	  var matchLengths [][]int
	  //<<Allocate space for match lengths, Pr. \ref{pr:mfd}>>
	  //<<Stream target rep against neighbors, Pr. \ref{pr:mfd}>>
	  lengthSets <- matchLengths
  }(neighborNames)
#+end_src
#+begin_export latex
To hold the match lengths, we allocate an array of integers for each
sequence that makes up the target representative.
#+end_export
#+begin_src go <<Allocate space for match lengths, Pr. \ref{pr:mfd}>>=
  for _, targetSeq := range targetSeqs {
	  n := len(targetSeq.Data())
	  lengths := make([]int, n)
	  matchLengths = append(matchLengths, lengths)
  }
#+end_src
#+begin_export latex
For the streaming step, we again iterate over each neighbor sequence,
while keeping track of the lengths via the index \ty{li}.
#+end_export
#+begin_src go <<Stream target rep against neighbors, Pr. \ref{pr:mfd}>>=
  li := 0
  for _, neighbor := range neighborNames {
	  p := *optN + "/" + neighbor
	  f, err := os.Open(p)
	  util.Check(err)
	  //<<Iterate over neighbor sequences, Pr. \ref{pr:mfd}>>
	  f.Close()
  }
#+end_src
#+begin_export latex
While iterating over the neighbors, we aggregate the nucleotides,
construct the corresponding suffix array, and stream the target rep
against it. Note that aggregation only works within a neighbor file,
not across multiple neighbors.
#+end_export
#+begin_src go <<Iterate over neighbor sequences, Pr. \ref{pr:mfd}>>=
  sc := fasta.NewScanner(f)
  for sc.ScanSequence() {
	  //<<Aggregate nucleotides, Pr. \ref{pr:mfd}>>
	  //<<Construct enhanced suffix array, Pr. \ref{pr:mfd}>>
	  //<<Stream target rep against enhanced suffix array, Pr. \ref{pr:mfd}>>
  }
#+end_src
#+begin_export latex
To aggregate nucleotides, we declare a variable for a sequence, and
two byte slices to accumulate sequence data and headers. Then we loop
as long as we have accumulated fewer than the maximum number of
nucleotides. Inside the loop we scan a sequence and append it.
#+end_export
#+begin_src go <<Aggregate nucleotides, Pr. \ref{pr:mfd}>>=
  s := sc.Sequence()
  d := s.Data()
  h := []byte(s.Header())
  for len(d) < mnl  && sc.ScanSequence() {
	  s = sc.Sequence()
	  h = append(h, '|')
	  h = append(h, []byte(s.Header())...)
	  d = append(d, s.Data()...)
	  li++
  }
#+end_src
#+begin_export latex
We convert the current neighbor sequence do upper case and convert it
to the corresponding enhanced suffix array. We have a choice here
between calculating the ESA from the forward and reverse strand, in
which case we stream only the forward strand of the target rep, or
calculating the ESA just from the forward strand, in which case we
stream both the forward and the reverse strand of the target rep.

To chose between these two scenarios, I wrote the program
\ty{testStream} that reads an input sequence of arbitrary length. It
then computes the ESA from that sequence and streams a random sequence
of identical length against that ESA, moving in steps of matching
prefixes. The ESA is either computed from the forward strand only, in
which case the forward and reverse strand of the second sequence is
streamed against it, or from the forward and reverse strand, in which
case only the forward strand of the second sequence is streamed
against the ESA.

Figure~\ref{fig:ts}A shows that the ESA from forward only is
approximately 50\% faster with sequence length 100 Mb. Even more
significantly, the memory requirement of forward only is approximately
80\% smaller with sequence length 100 Mb. So we calculate the ESA only
from the forward strand, which we convert to upper case beforehand.

\begin{figure}
  \begin{center}
    \begin{tabular}{cc}
      \textbf{A} & \textbf{B}\\
      \includegraphics{../makeFurDb/time} &
      \includegraphics{../makeFurDb/memory}
    \end{tabular}
  \end{center}
  \caption{}\label{fig:ts}
\end{figure}
#+end_export
#+begin_src go <<Construct enhanced suffix array, Pr. \ref{pr:mfd}>>=
  d = bytes.ToUpper(d)
  e := esa.MakeEsa(d)
#+end_src
#+begin_export latex
We import \ty{esa}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "github.com/evolbioinf/esa"
#+end_src
#+begin_export latex
We iterate over the sequences of the target rep. For each sequence we
match the forward and the reverse strands by calling the function
\ty{matchSeq}, which we still need to write.
#+end_export
#+begin_src go <<Stream target rep against enhanced suffix array, Pr. \ref{pr:mfd}>>=
  for i, targetSeq := range targetSeqs {
	  d := targetSeq.Data()
	  rev := false
	  matchSeq(d, e, matchLengths[i], rev)
	  d = revTargetSeqs[i].Data()
	  rev = true
	  matchSeq(d, e, matchLengths[i], rev)
  }
#+end_src
#+begin_export latex
Inside \ty{matchSeq}, we traverse the sequence in steps of matching
prefixes. Each match is analyzed and stored before we advance by its
length.
#+end_export
#+begin_src go <<Functions, Pr. \ref{pr:mfd}>>=
  func matchSeq(d []byte, e *esa.Esa, ml []int, rev bool) {
	  for i := 0; i < len(d); {
		  match := e.MatchPref(d[i:])
		  //<<Analyze match, Pr. \ref{pr:mfd}>>
		  //<<Store match, Pr. \ref{pr:mfd}>>
		  i += match.L
	  }
  }
#+end_src
#+begin_export latex
It is possible that our current target suffix has no matching
prefix. Then we still advance by one position, otherwise we'd we
caught in an infinite loop.
#+end_export
#+begin_src go <<Analyze match, Pr. \ref{pr:mfd}>>=
  if match.L == 0 {
	  match.L = 1
  }
#+end_src
#+begin_export latex
All matches are stored by their starting positions with respect to the
forward strand. If that position is currently occupied by a shorter
match, we record the longer one.
#+end_export
#+begin_src go <<Store match, Pr. \ref{pr:mfd}>>=
  p := i
  if rev {
	  p = len(d) - i - match.L
  }
  if ml[p] < match.L {
	  ml[p] = match.L
  }
#+end_src
#+begin_export latex
In the closer we wait until all working goroutines have finished and
then close the channel.
#+end_export
#+begin_src go <<Streaming closer, Pr. \ref{pr:mfd}>>=
  go func() {
	  wg.Wait()
	  close(lengthSets)
  }()
#+end_src
#+begin_export latex
The iterator that drives the goroutines picks two-dimensional slices
of match lengths from the channel. These are stored in a master set of
match lengths, which we construct first.
#+end_export
#+begin_src go <<Streaming iterator, Pr. \ref{pr:mfd}>>=
  matchLengths := make([][]int, 0)
  for _, ts := range targetSeqs {
	  ml := make([]int, len(ts.Data()))
	  matchLengths = append(matchLengths, ml)
  }
  for lengthSet := range lengthSets {
	  //<<Store match lengths, Pr. \ref{pr:mfd}>>
  }
#+end_src
#+begin_export latex
We always store the longer of two match lengths.
#+end_export
#+begin_src go <<Store match lengths, Pr. \ref{pr:mfd}>>=
  for i, lengths := range lengthSet {
	  for j, length := range lengths {
		  if matchLengths[i][j] < length {
			  matchLengths[i][j] = length
		  }
	  }
  }
#+end_src
#+begin_export latex
The match lengths we've now recorded come from potentially many
traversals of the target rep. We merge the results of these traversals
by noting at every position the length of the longest match implied.
#+end_export
#+begin_src go <<Merge match lengths, Pr. \ref{pr:mfd}>>=
  for _, ml := range matchLengths {
	  l := 0
	  for i := 0; i < len(ml); i++ {
		  if ml[i] > l {
			  l = ml[i]
		  }
		  ml[i] = l
		  l--
	  }
  }
#+end_src
#+begin_export latex
The match lengths are our means to get the ultimate result of this
calculation, the positions where matches end. So wherever in the array
of match lengths a match ends, we note a ``1'', ``0'' everywhere
else.
#+end_export
#+begin_src go <<Convert match lengths to match ends, Pr. \ref{pr:mfd}>>=
  for _, ml := range matchLengths {
	  for i := 0; i < len(ml); {
		  m := ml[i]
		  for j := 0; j < m-1; j++ {
			  ml[i+j] = 0
		  }
		  ml[i+m-1] = 1
		  i += m
	  }
  }
#+end_src
#+begin_export latex
We write the match ends as FASTA-formatted sequences to the file
\ty{e.fasta} in the database. For this we iterate over the target
sequences. For each target sequence, we print its header and iterate
over the match lengths, which now contain the match ends. To make the
printing nimble, we use a buffered writer.
#+end_export
#+begin_src go <<Write match ends, Pr. \ref{pr:mfd}>>=
  f, err = os.Create(*optD + "/e.fasta")
  util.Check(err)
  wr := bufio.NewWriter(f)
  for i, seq := range targetSeqs {
	  ml := matchLengths[i]
	  fmt.Fprintf(wr, ">%s\n", seq.Header())
	  //<<Iterate over match ends, Pr. \ref{pr:mfd}>>
  }
  err = wr.Flush()
  util.Check(err)
  f.Close()
#+end_src
#+begin_export latex
We import \ty{bufio}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "bufio"
#+end_src
#+begin_export latex
While iterating over the match ends, we print the corresponding lines
of zeros interspersed with ones. The last line requires special
attention.
#+end_export
#+begin_src go <<Iterate over match ends, Pr. \ref{pr:mfd}>>=
  for j := 0; j < len(ml); j++ {
	  c := '0'
	  if ml[j] == 1 {
		  c = '1'
	  }
	  fmt.Fprintf(wr, "%c", c)
	  if (j+1) % fasta.DefaultLineLength == 0 {
		  fmt.Fprintf(wr, "\n")
	  }
  }
  //<<Deal with last line, Pr. \ref{pr:mfd}>>
#+end_src
#+begin_export latex
We add a newline to the last line unless this has already been done.
#+end_export
#+begin_src go <<Deal with last line, Pr. \ref{pr:mfd}>>=
  if len(ml) % fasta.DefaultLineLength != 0 {
	  fmt.Fprintf(wr, "\n")
  }
#+end_src
#+begin_export latex
\subsection{Blast Database of Neighbors, \ty{n}}
We come to the last element of the \ty{fur} database, the Blast
database of the neighbors. To construct this, we run the external
program \ty{makeblastdb} and pipe the neighbor sequences to it. At
this point we visit every neighbor sequence. So we take the
opportunity to also count the total number of nucleotides they
contain, $l$, and the number of \ty{C}s and \ty{G}s, $g$. In
preparation for this we declare the variables $l$ and $g$.
#+end_export
#+begin_src go <<Write Blast database of neighbors to \ty{n}, Pr. \ref{pr:mfd}>>=
  fmt.Fprintf(os.Stderr, "making Blast database\n")
  cmd := exec.Command("makeblastdb", "-dbtype", "nucl",
	  "-out", *optD + "/n", "-title", "t")
  stdin, err := cmd.StdinPipe()
  util.Check(err)
  var l, g int
  go func() {
	  defer stdin.Close()
	  //<<Pipe neighbors to \ty{makeblastdb}, Pr. \ref{pr:mfd}>>
  }()
  _, err = cmd.Output()
  util.Check(err)
#+end_src
#+begin_export latex
We import \ty{exec}.
#+end_export
#+begin_src go <<Imports, Pr. \ref{pr:mfd}>>=
  "os/exec"
#+end_src
#+begin_export latex
We iterate over the sequences in each neighbor file and print each
sequence to the destination file. We also count the total number of
nucleotides, $l$, and the combined number of \ty{G} and \ty{C}, $g$.
#+end_export
#+begin_src go <<Pipe neighbors to \ty{makeblastdb}, Pr. \ref{pr:mfd}>>=
  for neighbor, _ := range neighbors {
	  p := *optN + "/" + neighbor
	  r, err := os.Open(p)
	  util.Check(err)
	  sc := fasta.NewScanner(r)
	  for sc.ScanSequence() {
		  seq := sc.Sequence()
		  //<<Update $l$ and $g$, Pr. \ref{pr:mfd})>>
		  fmt.Fprintf(stdin, "%s\n", seq)
	  }
  }
#+end_src
#+begin_export latex
We scan across the sequence data to count the residues.
#+end_export
#+begin_src go <<Update $l$ and $g$, Pr. \ref{pr:mfd})>>=
  d := bytes.ToUpper(seq.Data())
  for _, c := range d {
	  if c == 'A' || c == 'C' || c == 'G' || c == 'T' {
		  l++
		  if c == 'C' || c == 'G' {
			  g++
		  }
	  }
  }
#+end_src
#+begin_export latex
\subsection{Neighbor Statistics, \ty{n.txt}}
We write the number of nucleotides and the GC content to file.
#+end_export
#+begin_src go <<Write statistics of neighbors to \ty{n.txt}, Pr. \ref{pr:mfd}>>=
  w, err := os.Create(*optD + "/n.txt")
  util.Check(err)
  gc := float64(g) / float64(l)
  fmt.Fprintf(w, "length: %d\nGC-content: %f\n", l, gc)
  w.Close()
#+end_src
#+begin_export latex
We are done with \ty{makeFurDb}, time to test it.
\section{Testing}
Our program to test \ty{makeFurDb} has hooks for imports and the
testing logic.
#+end_export
#+begin_src go <<makeFurDb_test.go>>=
  package main

  import (
	  "testing"
	  //<<Testing imports, Pr. \ref{pr:mfd}>>
  )

  func TestMakeFurDb(t *testing.T) {
	  //<<Testing, Pr. \ref{pr:mfd}>>
  }
#+end_src
#+begin_export latex
We construct a set of tests and iterate over them.
#+end_export
#+begin_src go <<Testing, Pr. \ref{pr:mfd}>>=
  var tests []*exec.Cmd
  //<<Construct tests, Pr. \ref{pr:mfd}>>
  for i, test := range tests {
	  //<<Run test, Pr. \ref{pr:mfd}>>
  }
#+end_src
#+begin_export latex
We import \ty{exec}.
#+end_export
#+begin_src go <<Testing imports, Pr. \ref{pr:mfd}>>=
  "os/exec"
#+end_src
#+begin_export latex
Our first test is a run with default options, where we only set the
targets, neighbors and the database. We also allow the database to be
overwritten.
#+end_export
#+begin_src go <<Construct tests, Pr. \ref{pr:mfd}>>=
  p := "./makeFurDb"
  a := "targets"
  n := "neighbors"
  d := "test.db"
  test := exec.Command(p, "-t", a, "-n", n, "-d", d, "-o")
  tests = append(tests, test)
#+end_src
#+begin_export latex
In our second test we also pick \ty{t2.fasta} as the target
representative.
#+end_export
#+begin_src go <<Construct tests, Pr. \ref{pr:mfd}>>=
  r := "t2.fasta"
  test = exec.Command(p, "-t", a, "-n", n, "-d", d, "-o",
	  "-r", r)
  tests = append(tests, test)
#+end_src
#+begin_export latex
In our third test and last test we also set the number of threads to 1.
#+end_export
#+begin_src go <<Construct tests, Pr. \ref{pr:mfd}>>=
  test = exec.Command(p, "-t", a, "-n", n, "-d", d, "-o",
	  "-r", r, "-T", "1")
  tests = append(tests, test)
#+end_src
#+begin_export latex
We run a test and compare the result we get, that is, the database
file \ty{e.fasta}, with the result we want, which is stored in the
files \ty{r1.txt}, \ty{r2.txt}, and so on.
#+end_export
#+begin_src go <<Run test, Pr. \ref{pr:mfd}>>=
  _, err := test.Output()
  if err != nil { t.Error(err) }
  get, err := os.ReadFile(d + "/e.fasta")
  if err != nil { t.Error(err) }
  f := "r" + strconv.Itoa(i+1) + ".txt"
  want, err := os.ReadFile(f)
  if err != nil { t.Error(err) }
  if !bytes.Equal(get, want) {
	  t.Errorf("get:\n%s\nwant:\n%s\n", get, want)
  }
#+end_src
#+begin_export latex
We import \ty{os}, \ty{strconv}, and \ty{bytes}.
#+end_export
#+begin_src go <<Testing imports, Pr. \ref{pr:mfd}>>=
  "os"
  "strconv"
  "bytes"
#+end_src
