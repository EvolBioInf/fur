#+begin_src latex  
  \section{Introduction}
  To demonstrate \ty{fur}, we first apply it to simulated data, then to
  the \emph{E. coli} genomes introduced in Figure~\ref{fig:eco}.

  \section{Apply \ty{fur} to Simulated Data}
  We make a directory for this tutorial and change into it.
#+end_src
#+begin_src sh <<tut>>=
  mkdir tutorial
  cd tutorial
#+end_src
#+begin_src latex
  We use the program \ty{stan}\footnote{\ty{github.com/evolbioinf/stan}}
  to simulate target and neighbor sequences. To make the simulation
  reproducible, we seed the random number generator of \ty{stan} with
  the \ty{-s} option.
#+end_src
#+begin_src sh <<tut>>=
  stan -s 3
#+end_src
#+begin_src latex
  This has generated two directories, \ty{targets} and \ty{neighbors},
  with ten sequences each.
#+end_src
#+begin_src sh <<tut>>=
  ls targets/ | wc -l
  ls neighbors/ | wc -l
#+end_src
#+begin_src latex
  We visualize the relationship of the target and neighbor sequences by
  calculating their pairwise distances with
  \ty{phylonium}~\cite{klo20:phy}. We summarize these distances into the
  phylogeny in Figure~\ref{fig:phy}, where the targets, marked by prefix
  ``t'', are well separated from the neighbors, marked by prefix
  ``n''. The program \ty{phylonium} is part of common package
  repositories like \ty{apt}, the programs \ty{nj}, \ty{midRoot}, and
  \ty{plotTree} are part of the biobox collection of bioinformatics
  tools\footnote{\ty{github.com/evolbioinf/biobox}}.
  \begin{figure}
    \begin{center}
      \includegraphics{../tutorial/phy}
    \end{center}
    \caption{Phylogeny of a random sample of target and neighbor
      sequences generated with \ty{stan}.}\label{fig:phy}
  \end{figure}
#+end_src
#+begin_src sh <<tut>>=
  phylonium targets/* neighbors/* |
      nj |
      midRoot |
      plotTree
#+end_src
#+begin_src latex
  The target sequences are each 10 kb long, giving a total of 100 kb. We
  count the target nucleotides using \ty{cres}, which is also part of
  the biobox.
#+end_src
#+begin_src sh <<tut>>=
  cres targets/*
#+end_src
#+begin_src latex
  The neighbor sequences total only 90 kb, as they lack 1 kb right in
  the middle. In other words, the region 4501--5500 in the targets is
  the marker we'd like to identify.
#+end_src
#+begin_src sh <<tut>>=
  cres neighbors/*
#+end_src
#+begin_export latex
We convert the target and neighbor sequences into the database
\ty{test.db}.
#+end_export
#+begin_src sh <<tut>>=
  makeFurDb -t targets -n neighbors -d test.db
#+end_src
#+begin_export latex
We run \ty{fur} on the new database \ty{test.db}. This returns a
summary of the run and the marker fragment.
#+end_export
#+begin_src sh <<tut>>=
  fur -d test.db/
#+end_src
#+begin_export latex
\begin{verbatim}
  Step           Sequences  Length  Ns
  -------------  ---------  ------  --
  Subtraction 1          1    1040   0
  Intersection           1    1038   5
  Subtraction 2          1    1038   5
>t4 (4485..5522)   5  126  165  363  620  762
GCAATCCTGCATCGACTGTGGCTCTGGGAGGTCCGACCAAGTGCTACTGCGGCATTAG...
GTATACATGGACAAGGATCCCAACTGAATCTGAAAGCAAGGTAATTCTAGGACGTAGA
\end{verbatim}
#+end_export
#+begin_export latex
As expected, the marker is approximately 1 kb long. Its header consist
of the accession, followed by the interval, followed by the number of
mutations among the target sequences, followed by the positions of the
mutations. The coordinates of the fragment interval are given with
respect to the target representative, which is saved as \ty{r.fasta}
in the database. So we can double-check a fragment by cutting it from
the target representative using the program \ty{cutSeq} form the
biobox.
#+end_export
#+begin_src sh <<tut>>=
  cutSeq -r 4485-5522 test.db/r.fasta
#+end_src
#+begin_src latex
  To make the process of marker selection more transparent, the \ty{-u}
  option allows printing of the unique regions found in the first
  subtraction step.
#+end_src
#+begin_src sh <<tut>>=
  fur -d test.db -u > unique1.fasta
#+end_src
#+begin_export latex
The file \ty{unique1.fasta} now contains the 1040 bp isolated in the
first subtraction step.
#+end_export
#+begin_src sh <<tut>>=
  cres unique1.fasta
#+end_src
#+begin_src latex
  Similarly, the regions identified by the intersection step can be
  printed using the \ty{-U} option.
#+end_src
#+begin_src sh <<tut>>=
  fur -d test.db -U > unique2.fasta
#+end_src
#+begin_src latex
  The file \texttt{unique2.fasta} now contains the 1038 bp returned by
  the intersection step.
#+end_src
#+begin_src sh <<tut>>=
  cres unique2.fasta
#+end_src
#+begin_export latex
Let's now explore the resource consumption of \ty{makeFurDb}. Its
expected run time is simply proportional to the number of neighbor
sequences---given constant target size. However, its expected memory
consumption is proportional to the length of the longest neighbor
sequence.

In theory, the memory consumption of \ty{makeFurDb} is proportional to
the length of the longest neighbor sequence. Since the sequences
simulated by \ty{stan} all have the same length, the memory
consumption of \ty{makeFurDb} should be independent of the number of
neighbor sequences. To show this is actually the case, we start with
simulated neighbors and targets of 5 Mb, the length of an
\emph{E. coli} genome.
#+end_export
#+begin_src sh <<tut>>=
  stan -o -l 5000000
#+end_src
#+begin_export latex
We now need to measure the run time and the memory consumption of
\ty{makeFurDb}. Time measurements are routinely carried out using the
\ty{time} command built into the shell. However, it cannot measure
memory, while the \ty{time} command of the system can. So we run that
by giving its explicit path, and ask it to print the user time and the
peak memory. We find that \ty{makeFurDb} runs in 15 s and occupies 827
MB memory.
#+end_export
#+begin_src sh <<tut>>=
  /usr/bin/time -f "%Us\t%Mkb" makeFurDb -t targets/ \
		-n neighbors/ -d test.db/ -o
#+end_src
#+begin_export latex
By comparison, the resource requirement of \ty{fur} is neglegible with
0.1 s and 149 MB.
#+end_export
#+begin_src sh <<tut>>=
  /usr/bin/time -f "%Us\t%Mkb" fur -d test.db/
#+end_src
#+begin_export latex
Let's double the size of the neighborhood.
#+end_export
#+begin_src sh <<tut>>=
  stan -o -l 5000000 -n 20
#+end_src
#+begin_export latex
As expected, \ty{makeFurDb} now takes approximately twice the run
time, 28 s, but still only occupies 833 MB of memory.
#+end_export
#+begin_src sh <<tut>>=
  /usr/bin/time -f "%Us\t%Mkb" makeFurDb -t targets/ \
		-n neighbors/ -d test.db/ -o
#+end_src
#+begin_export latex
We finish the simulation part of the tutorial by deleting the database
\ty{test.db} and its source directories.
#+end_export
#+begin_src sh <<tut>>=
  rm -r test.db targets neighbors
#+end_src
#+begin_src latex
  \section{Apply \texttt{fur} to Real Data}\label{sec:furTut}
  We next use \texttt{fur} to find regions specific to the pathogenic
  \emph{E. coli} strain ST131 in the example data shown in
  Figure~\ref{fig:eco}. The first step is to download the data from the
  \ty{fur} website and to unpack it.
#+end_src
#+begin_src sh <<tut>>=
  wget guanine.evolbio.mpg.de/fur/eco105.tar.gz
  tar -xvzf eco105.tar.gz
#+end_src
#+begin_src latex
  This generates two directories of genomes in FASTA format,
  \texttt{targets} with 98 genomes, and \texttt{neighbors} with
  seven. These are converted to a \texttt{fur} database using
  \texttt{makeFurDb}  (Chapter~\ref{ch:makeFurDb}),
  which takes approximately 12 s.
#+end_src
#+begin_src sh <<tut>>=
  makeFurDb -t targets -n neighbors -d eco105.db
#+end_src
#+begin_src latex
  We apply \texttt{fur} to this database, which takes roughly three
  second. The marker candidates are stored in \texttt{eco105.fasta}.
  The progress information printed by \ty{fur} lists the three steps of
  the algorithm and the number of sequences and nucleotides contained in
  the template set after each step. So the initial subtraction step
  uncovers 465 sequences totaling 291.0 kb and containing no unknown
  nucleotide, \texttt{N}. After intersecting the remaining targets with
  the output from the first subtraction, 239 sequences with 70.1 kb
  remain. These sequences contain 574 \ty{N}s. The second and last
  subtraction step yields 233 regions with 69.3 kb and 508 \ty{N}s.
#+end_src
#+begin_src sh <<tut>>=
  fur -d eco105.db/ > eco105.fasta
#+end_src
#+begin_src latex
  \begin{verbatim}
    Step           Sequences  Length   Ns
    -------------  ---------  ------   --
    Subtraction 1        465  291003    0
    Intersection         239   70063  574
    Subtraction 2        233   69310  508
  \end{verbatim}

  The program \ty{fur} has a number of parameters that can affect its
  output. We begin our investigation of these parameters with the
  quantile of the match length distribution. This is calculated from the
  combined length and GC content of the neighbor sequences, two
  quantities stored in the database file \ty{n.txt}.
#+end_src
#+begin_src sh <<tut>>=
  cat eco105.db/n.txt
#+end_src
#+begin_export latex
\begin{verbatim}
length: 35524975
GC-content: 0.505901
\end{verbatim}
#+end_export
#+begin_export latex
We use the program \ty{madis} to convert the length and GC content of
the neighbors into the match length corresponding to the 0.01 quantile
of the match length distribution, which turns out to be 11.
#+end_export
#+begin_src sh <<tut>>=
  madis -l 35524975 -g 0.505901 -q 0.01
#+end_src
#+begin_export latex
To find out the quantiles that correspond to other match lengths, we
run \ty{maid} without the switch for a specific quantile.
#+end_export
#+begin_src sh <<tut>>=
  madis -l 35524975 -g 0.505901 | head
#+end_src
#+begin_export latex
\begin{verbatim}
#x  PDF(x)     CDF(x)
 8  2.99e-109  2.99e-109
 9  5.32e-29   5.32e-29
10  5.31e-08   5.31e-08
11  0.0146     0.0146
12  0.332      0.347
13  0.421      0.767
14  0.169      0.936
15  0.0477     0.984
16  0.0123     0.996
\end{verbatim}
The cumulative density function, CDF, tells us that match length 12
corresponds to quantiles $0.0146 < q < 0.347$. However, when we run
\ty{fur} with a quantile out of this interval, say 0.1, nothing
changes. So we try a quantile for the 13-interval, $0.347 < q <
0.767$, say 0.4. Now our \ty{fur} output increases by over 50\% to 110
kb. We throw this output away in our example analysis, but in a real
analysis we'd of course keep it.
#+end_export
#+begin_src sh <<tut>>=
  fur -q 0.4 -d eco105.db/ > /dev/null
#+end_src
#+begin_export latex
\begin{verbatim}
  Step           Sequences  Length    Ns
  -------------  ---------  ------    --
  Subtraction 1        953  385684     0
  Intersection         527  112063  1235
  Subtraction 2        511  110205   906
\end{verbatim}

So it might be useful to experiment with various match lengths via the
\ty{-q} parameter. Another parameter that can affect the yield is the
length of the sliding window, \ty{-w}. For example, if we set it from
default 80 to 70, the output again grows, this time to 152.8 kb.
#+end_export
#+begin_src sh <<tut>>=
  fur -q 0.4 -w 70 -d eco105.db/ > /dev/null
#+end_src
#+begin_export latex
\begin{verbatim}
  Step           Sequences  Length    Ns
  -------------  ---------  ------    --
  Subtraction 1       1412  464155     0
  Intersection         786  155005  1547
  Subtraction 2        765  152839  1412
\end{verbatim}

We time this \ty{fur} run to find that it takes 2.7 s.
#+end_export
#+begin_src sh <<tut>>=
  /usr/bin/time -f "%Us\t%Mkb" fur -q 0.4 -w 70 \
		-d eco105.db/ > /dev/null
#+end_src
#+begin_src latex
  Since in our example a \ty{fur} run takes under three seconds, we can
  iterate over a range of window sizes using a \ty{do} loop, for example
  the 61 window sizes 60--120. This should take no more than three
  minutes.

  The result of the \ty{do} loop is plotted with \ty{plotLine} from the
  biobox. Figure~\ref{fig:yie} shows that for short window lengths the
  yield fluctuates strongly, but this dampens out for longer windows.

  \begin{figure}
    \begin{center}
      \includegraphics{../tutorial/yield}
    \end{center}
    \caption{The yield of \ty{fur} as a function of the window length.}\label{fig:yie}
  \end{figure}
#+end_src

#+begin_src sh <<tut>>=
  for a in $(seq 60 120)
  do
      echo -n $a " "
      fur -w $a -q 0.4 -d eco105.db/ 2>&1 |
	  grep "Subtraction 2" |
	  awk '{print $4}'
  done | plotLine
#+end_src
#+begin_src latex
  The last two parameters to mention affect the sensitivity of the
  second subtraction step, which is implemented as a BLAST search. This
  can be modulated via its mode (algorithm) and $E$-value. The mode of
  the BLAST-search among the neighborhood sequences is by default the
  sensitive ``blastn'' mode. Option \texttt{-m} switches it to the
  faster and less sensitive ``megablast'' mode. However, in the case of
  the 105 \emph{E. coli} genomes this makes no difference. The $E$-value
  is by default $10^{-5}$. Setting it to some other value again makes no
  difference in this case, but may well do so in other analyses.

  \section{Making Primers, \texttt{fur2prim} \& \texttt{prim2fasta}}
  We design primers with the program \ty{primer3}~\cite{unt12:pri}. So
  we convert the output of \ty{fur} into input to \ty{primer3}.
#+end_src
#+begin_src sh <<tut>>=
  fur2prim eco105.fasta > prim.txt
#+end_src
#+begin_src latex
  Running \texttt{primer3} on \ty{prim.txt} takes approximately half a
  minute.
#+end_src
#+begin_src sh <<tut>>=
  primer3_core prim.txt > prim.out
#+end_src
#+begin_src latex
  Each entry in \ty{prim.out} is terminated by \ty{=}, so we can count
  the 233 entries.
#+end_src
#+begin_src sh <<tut>>=
  grep '^=' prim.out | wc -l
#+end_src
#+begin_src latex
  Each of these entries contains up to 5 primers. The parameter
  \begin{verbatim}
  PRIMER_PAIR_NUM_RETURNED
  \end{verbatim}
  gives us the number of primers per entry, so we can count the total
  number of primers returned, 1040.
#+end_src
#+begin_src sh <<tut>>=
  grep PRIMER_PAIR_NUM_RETURNED prim.out |
      tr '=' '\t' |
      awk '{s+=$2}END{print s}'
#+end_src
#+begin_src latex
  We rank our 1040 primers by quality. Each primer pair has a penalty
  denoted as
  \begin{verbatim}
  PRIMER_PAIR_X_PENALTY
  \end{verbatim}
  where X is 0 for the best pair of a particular template, 1 for the
  second best, and so on. We can extract and sort these values to
  discover the overall best primer pair, which has penalty 0.008130.
#+end_src
#+begin_src sh <<tut>>=
  grep PRIMER_PAIR_0_PENALTY prim.out |
      tr '=' '\t' |
      sort -k 2 -n |
      head -n 1
#+end_src
#+begin_src latex
  We can now load \ty{prim.out} into a text editor and search for the
  primers with penalty 0.008130.

  \section{Checking Primers, \texttt{checkPrim}}\label{sec:checkTut}
  We download an example primer pair for SARS-CoV-2 from the \ty{fur}
  web site.
#+end_src
#+begin_src sh <<tut>>=
  wget guanine.evolbio.mpg.de/fur/p1.fa
#+end_src
#+begin_src latex
  Our aim is to calculate the specificity and sensitivity of the two
  primers contained in \ty{p1.fa}. For this we first download a BLAST
  database containing targets and non-targets.
#+end_src
#+begin_src sh <<tut>>=
  <<Download BLAST database, Ch. \ref{ch:tut}>>
  <<Calculate sensitivity, Ch. \ref{ch:tut}>>
  <<Calculate specificity, Ch. \ref{ch:tut}>>
#+end_src
#+begin_src latex
  Since we are investigating a pair of SARS-CoV-2 primers, we could use
  the current version of the Betacoronavirus BLAST database for testing.
  We would download this and the taxonomy database, which we need to
  distinguish between targets and non-targets, using two calls to the
  program \ty{update\_blastdb.pl} from the BLAST package.

  \begin{verbatim}
  update_blastdb.pl --decompress Betacoronavirus
  update_blastdb.pl --decompress taxdb
  \end{verbatim}

  However, the current Betacoronavirus database is rather large (and
  growing rapidly), so we use a much smaller version from 2020, which we
  download from the \ty{fur} website and unpack into the directory
  \ty{betacor}, together with the corresponding taxonomy database.
#+end_src
#+begin_src sh <<Download BLAST database, Ch. \ref{ch:tut}>>=
  wget http://guanine.evolbio.mpg.de/fur/betacor.tgz
  tar -xvzf betacor.tgz
  ls betacor/
#+end_src
#+begin_src latex
  The database \ty{Betacoronavirus} contains 8271 entries.
#+end_src
#+begin_src sh <<Download BLAST database, Ch. \ref{ch:tut}>>=
  blastdbcmd -db betacor/Betacoronavirus -entry all |
      grep '^>' |
      wc -l
#+end_src
#+begin_src latex
  The sensitivity of a set of primers is the fraction of amplified
  target sequences. To calculate this fraction, we need to identify the
  SARS-CoV-2 virus by its taxon-ID, which we look up using the using the
  ENTREZ programs \ty{esearch} and \ty{efetch}. We find that the
  taxon-ID of SARS-CoV-2 is 2697049.
#+end_src
#+begin_src sh <<Calculate sensitivity, Ch. \ref{ch:tut}>>=
  esearch -db taxonomy -query "SARS-CoV-2" |
      efetch -format docsum
#+end_src
#+begin_src latex
  There are 567 SARS-CoV-2 entries in the database.
#+end_src
#+begin_src sh <<Calculate sensitivity, Ch. \ref{ch:tut}>>=
  blastdbcmd -db betacor/Betacoronavirus -taxids 2697049 |
      grep '^>' |
      wc -l
#+end_src
#+begin_src latex
  We find 487 hits with our primers and save their accessions as a
  sorted list.
#+end_src
#+begin_src sh <<Calculate sensitivity, Ch. \ref{ch:tut}>>=
  checkPrim -v query=p1.fa \
	    -v db=betacor/Betacoronavirus \
	    -v taxids=2697049 |
      cut -f 3 |
      tail -n +2 |
      sort > obs.txt
#+end_src
#+begin_export latex
487 hits is much less than the 567 SARS-CoV-2 entries in the
database. However, we don't know how many of the 567 entries contain
the marker. What we do know, is that every complete genome sequence
should contain the marker. To determine the fraction of target genomes
we found, we determine the accessions of the 438 complete genome
sequences we expect to find, remove their version information, and
store them as a sorted list. This is the list of accessions we expect
to find.
#+end_export
#+begin_src sh <<Calculate sensitivity, Ch. \ref{ch:tut}>>=
  blastdbcmd -db betacor/Betacoronavirus -taxids 2697049 |
      grep '^>' |
      grep 'complete genome' |
      awk '{print $1}' |
      sed 's/>//;s/\.[0-9]*//' |
      sort > exp.txt
#+end_src
#+begin_src latex
  The sensitivity is the size of the join between observed and expected
  accessions divided by 438, which is 1. So the primers in \ty{p1.fa}
  are maximally sensitive.
#+end_src
#+begin_src sh <<Calculate sensitivity, Ch. \ref{ch:tut}>>=
  join exp.txt obs.txt |
      wc -l
#+end_src
#+begin_src latex
  The specificity is one minus the fraction of off-target
  amplifications. So we print all hits to sequences not classified as
  SARS-CoV-2. There are none, which means the specificity is also 1.
#+end_src
#+begin_src sh <<Calculate specificity, Ch. \ref{ch:tut}>>=
  checkPrim -v query=p1.fa \
	    -v db=betacor/Betacoronavirus \
	    -v negativeTaxids=2697049
#+end_src
#+begin_src latex
  This investigation of sensitivity and specificity should be repeated
  with a larger database, ideally the complete collection of known
  nucleotide sequences, \texttt{nt}. The full list of available
  databases is show by
  \begin{verbatim}
  update_blastdb.pl --showall
  \end{verbatim}
#+end_src
